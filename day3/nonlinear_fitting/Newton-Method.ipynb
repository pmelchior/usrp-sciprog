{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "We're often interested in the best-fitting model to some data. On Day 3, we introduced the concept of a likelihood and least-squares fitting. For linear models, we can do that in one step because the problem is uniquely determined. Today we will introduce how to fit functions that have non-linear parameters.\n",
    "\n",
    "\n",
    "## Gradients!\n",
    "\n",
    "We will assume the we know the likelihood $\\mathcal{L}$ (e.g. a Gaussian), which means that we have a function that is maximized with the choice of good parameters. The function we normally work with is\n",
    "\n",
    "$$\n",
    "f(x) = - \\log\\mathcal{L}(x\\mid\\mathcal{D})\n",
    "$$\n",
    "\n",
    "which we then minimize. The log is there to remove the exponentials in many likelihoods. For example, for the ordinary least-squares solution, $f=\\chi^2$.\n",
    "\n",
    "> Mind the sign!\n",
    "> It's *very* common to write down a model, optimize it, and then get some nonsense fit from the Minimum-Likelihoodâ„¢ parameters.\n",
    "\n",
    "The variable $x$ stands for the parameter we want to find the optimal value for. Notice that we don't require it to have any specific relation (for instance linear). Instead, we will demand that $f(x)$ represents a well-behaved function: we can expect derivatives of $f$ to exist everywhere in the region of interest. We can thus write down the **Taylor series** expansion for $f$ about some point $x_0$:\n",
    "\n",
    "$$\n",
    "f (x) = f (x_0) + g(x_0) (x - x_0) + \\frac{1}{2} H(x_0) (x - x_0)^2 + \\mathcal{O}((x-x_0)^3)\n",
    "$$\n",
    "\n",
    "where $g$ is the gradient, i.e. $g \\equiv df(x)/dx$, and the **Hessian** $H$ is $H \\equiv d^2 f(x) / dx^2$.\n",
    "\n",
    "Although we don't know anything a priori about the convergence of this series, it is clear that as the distance $x - x_0$ becomes smaller, the higher-order terms become less important.\n",
    "\n",
    "The first term of the above series is constant, so it will not tell much about where to look for a minimum. The second term is proportional to the gradient, telling in which direction the function is decreasing fastest, but it doesn't tell us what step size to take.\n",
    "\n",
    "A first-order gradient descent method thus is typically a fixed-point iteration of the kind\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\lambda_t g(x_t)\n",
    "$$\n",
    "\n",
    "At iteration $t$, it goes downhill by a certain amount $\\lambda_t$, which yet needs to be determined; setting it properly may require experience in the dark arts.\n",
    "\n",
    "\n",
    "The third, or quadratic term describes a parabolic behavior and is therefore the lowest-order term to predict a minimum. Unlike $g$, we can expect $H$ to be roughly constant over small regions because it's variations are of higher-order (and in the case of a true parabola: identically zero).\n",
    "\n",
    "Thus second-order gradient descent (also called **Newton methods**) have fixed-point iterations of the form\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - H^{-1}(x_t) g(x_t)\n",
    "$$\n",
    "\n",
    "We'll see why in a minute. This means that the optimal step size for a quadratic approximation of the function $f$ is given by the inverse curvature of $f$. That sounds intuitive enough, but let's have a picture anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# two parabolae\n",
    "f = lambda x, c: c*x**2\n",
    "c = 1, 0.5\n",
    "\n",
    "# initial point\n",
    "x_ = -0.75\n",
    "y_ = [ f(x_, ci) for ci in c ]\n",
    "\n",
    "# compute gradient and hessian\n",
    "g = [ 2*c[i]*x_ for i in range(2) ]\n",
    "H = [ 2*c[i] for i in range(2) ]\n",
    "\n",
    "# Newton step\n",
    "x__ = [ x_ - 1/H[i]*g[i] for i in range(2) ]\n",
    "y__ = [ f(xi, ci) for xi, ci in zip(x__, c) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": "true",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "x = np.linspace(-1,1,100)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "axes[0].plot(x, f(x, c[0]))\n",
    "axes[1].plot(x, f(x, c[1]))\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[0].scatter([x_, x__[0]], [y_[0], y__[0]], c=['k', 'w'], ec='k', s=100, zorder=10)\n",
    "axes[0].plot([x_, x_, x__[0]], [y_[0], y__[0], y__[0]], c='k')\n",
    "axes[1].scatter([x_, x__[1]], [y_[1], y__[1]], c=['k', 'w'], ec='k', s=100, zorder=10)\n",
    "axes[1].plot([x_, x_, x__[1]], [y_[1], y__[1], y__[1]], c='k')\n",
    "axes[0].text(0,0.25,'$f(x)=x^2,\\,H=2$', ha='center', size=16)\n",
    "axes[1].text(0,0.25,'$f(x)=x^2/2,\\,H=1$', ha='center', size=16)\n",
    "axes[0].text(x_/2, x__[0]-0.01, '$\\Delta x$', ha='center', va='top', size=16)\n",
    "axes[1].text(x_/2, x__[1]-0.01, '$\\Delta x$', ha='center', va='top', size=16)\n",
    "axes[0].text(x_-0.01, y_[0]/2, '$\\Delta y$', ha='right', va='center', size=16)\n",
    "axes[1].text(x_-0.01, y_[1]/2, '$\\Delta y$', ha='right', va='center', size=16)\n",
    "axes[0].set_ylim(-0.2,1.1)\n",
    "axes[1].set_ylim(-0.2,1.1)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having different slopes at the starting position (filled circle), the Newton scheme performs only a single step (open circle) to move to the exact minimum, from any starting position, *if the function is quadratic*. This is even more useful because \n",
    "\n",
    "> Any smooth function close to its minimum looks like a quadratic function!\n",
    "\n",
    "That's a consequence of the Taylor expansion because the first-order term $g$ vanishes close to the minimum, so all deviations from the quadratic form are of order 3 or higher in $x-x_0$.\n",
    "\n",
    "So, why doesn't everyone compute the Hessian for optimization. Well, it's typically expensive to compute a second derivative. And in $d$ dimensions (one for each parameter), the Hessian is a matrix with $d(d+1)/2$ elements. This is why there are several **quasi-Newton methods** like [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm), that accumulate information from previous iterations into an estimate of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method for finding a root\n",
    "\n",
    "[Newton's method](https://en.wikipedia.org/wiki/Newton's_method) was initially designed to find the root of a function, not its minimum. So, let's find out how these two are connected.\n",
    "\n",
    "The central idea is to approximate $f$ by its tangent at some initial position $x_0$:\n",
    "\n",
    "$$\n",
    "y =  f(x_0) + g(x_0) (x-x_0)\n",
    "$$\n",
    "\n",
    "As we can see in this animation from Wikipedia, the $x$-intercept of this line is then closer to the root than the starting position $x_0$:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/NewtonIteration_Ani.gif/600px-NewtonIteration_Ani.gif)\n",
    "\n",
    "That is, we need to solve the linear relation\n",
    "\n",
    "$$\n",
    "f(x_0) + g(x_0) (x-x_0) = 0\n",
    "$$\n",
    "\n",
    "for $x$ to get the updated position. In 1D: $x_1 = x_0 - f(x_0)/g(x_0)$. Repeating this sequence\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\frac{f(x_t)}{g(x_t)}\n",
    "$$\n",
    "\n",
    "will yield a fixed point, which is the root of $f$ *if one exists in the vicinity of $x_0$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def newtons_method(f, df, x0, tol=1E-6):\n",
    "    x_n = x0\n",
    "    i = 0\n",
    "    while abs(f(x_n)) > tol:\n",
    "        print(f\"interation {i}\")\n",
    "        x_n = x_n - f(x_n)/df(x_n)\n",
    "        i += 1\n",
    "    return x_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a function\n",
    "\n",
    "As the maximum and minimum of a function are defined by $f'(x) = 0$, we can use Newton's method to find extremal points by applying it to the first derivative. That's the origin for the Newton update formula above:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - H^{-1}(x_t) \\ g(x_t)\n",
    "$$\n",
    "\n",
    "Let's try this with a simply function with known minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define a test function\n",
    "def f(x):\n",
    "    return (x-3)**2 - 9\n",
    "\n",
    "def df(x):\n",
    "    return 2*(x-3)\n",
    "\n",
    "def df2(x):\n",
    "    return 2.\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, f(x))\n",
    "plt.axhline(0, c=\"k\", ls=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "root = newtons_method(f, df, x0=0.1)\n",
    "print (f\"root {root}, f(root) = {f(root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimum = newtons_method(df, df2, x0=0.1)\n",
    "print (f\"minimum {minimum}, f'(minimum) = {df(minimum)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important qualifier in the statement about fixed points: **a root needs to exist in the vicinity of $x_0$!** If that's not the case, `newtons_method` will not terminate and come back with a result. You'd have to manually interrupt the execution of this method (Jupyer Tip: click \"Interrupt Kernel\"),\n",
    "\n",
    "With a little more defensive programming we can make sure that the function will terminate after a given number of iterations. We'll also take out the print statement to make it faster and less messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def newtons_method2(f, df, x0, tol=1E-6, maxiter=100000):\n",
    "    x_n = x0    \n",
    "    for _ in range(maxiter):\n",
    "        x_n = x_n - f(x_n)/df(x_n)\n",
    "        if abs(f(x_n)) < tol:\n",
    "            return x_n\n",
    "        \n",
    "    raise RuntimeError(\"Failed to find a minimum within {} iterations \".format(maxiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "newtons_method2(f, df, x0=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy.optimize\n",
    "\n",
    "scipy comes with a pretty feature-rich [optimization package](https://docs.scipy.org/doc/scipy/reference/optimize.html), for one- and multi-dimensional optimization. As so often, it's better (as in faster and more reliable) to leverage exisiting and battle-tested code than to try to implement it yourself.\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "Find the minimum of `f` with `scipy.optimize.minimize_scalar`. Look up the various arguments to function in the documentation (either online or by typing `scipy.optimize.minimize_scalar?`) and choose appropriate inputs. When done, visualize your result to confirm its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "To make this more interesting, we'll create a new  multi-dimensional function that resembles `f`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, p):\n",
    "    return np.sum(np.abs(x-3)**p, axis=-1) - 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2D, find the minimum of `h` for `p=2` with `scipy.optimimze.minimize`. Note that you have not been given a derivative of `h`. You can choose to compute it analytically, or see if `minimize` has options that allow you to work without.\n",
    "\n",
    "When done, visualize your result to confirm its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Experiment what happens if $p<1$. Do you understand why that happens? Can it be avoided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
